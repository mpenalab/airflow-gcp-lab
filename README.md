# Pipeline de Datos con Apache Airflow y Google Cloud Platform

**Estudiante:** Miguel Angel PeÃ±a Carrillo

---

## ğŸ“‹ DescripciÃ³n del Pipeline

Este proyecto implementa un pipeline completo de ingenierÃ­a de datos utilizando **Apache Airflow** orquestado con **Docker Compose** e integrado con servicios de **Google Cloud Platform (GCP)**. El pipeline automatiza el flujo de datos desde su generaciÃ³n hasta su transformaciÃ³n y almacenamiento en un data warehouse.

### Arquitectura del Pipeline

El pipeline consta de los siguientes componentes:

1. **Sales API**: Servicio REST que genera datos de ventas y los publica en Google Cloud Pub/Sub
2. **Google Cloud Pub/Sub**: Sistema de mensajerÃ­a que recibe y almacena eventos de ventas
3. **Apache Airflow**: Orquestador que coordina las siguientes tareas:
   - ğŸ“¥ ExtracciÃ³n de mensajes desde Pub/Sub
   - ğŸ”„ Procesamiento y transformaciÃ³n de datos
   - â˜ï¸ Carga de datos a Google Cloud Storage (GCS)
   - ğŸ“Š Ingesta de datos a BigQuery (tabla raw)
   - ğŸ§® TransformaciÃ³n y agregaciÃ³n en BigQuery (tabla summary)

### Flujo de Datos

```
Sales API â†’ Pub/Sub â†’ Airflow DAG â†’ GCS â†’ BigQuery (Raw) â†’ BigQuery (Summary)
```

El DAG de Airflow (`sales_pipeline_gcp`) ejecuta las siguientes tareas en secuencia:

- **pull_pubsub_messages**: Extrae hasta 10 mensajes de la suscripciÃ³n `sales-sub`
- **process_and_upload_gcs**: Procesa los mensajes y los sube a GCS en formato NDJSON
- **load_to_bq_raw**: Carga los datos desde GCS a BigQuery en la tabla `sales_raw`
- **transform_summary_table**: Crea tabla agregada `sales_summary` con mÃ©tricas por producto y fecha

---

## ğŸš€ Instrucciones de EjecuciÃ³n

### Prerrequisitos

- Docker Desktop instalado y en ejecuciÃ³n
- Credenciales de Google Cloud Platform (`google_credentials.json`)
- Archivo `.env` configurado con las variables de entorno necesarias

### Pasos para Ejecutar el Proyecto

1. **Clonar el repositorio y navegar al directorio del proyecto**
   ```bash
   cd "c:\Users\cmigu\Desktop\Road to Data Engineer\MÃ³dulo 6 - IngenierÃ­a de Datos Avanzada con Python\airflow-gcp-lab\airflow-gcp-lab\lab_final"
   ```

2. **Verificar que los archivos de configuraciÃ³n estÃ©n presentes**
   - `google_credentials.json` en la raÃ­z del proyecto
   - `.env` con las variables `GCP_PROJECT_ID` y `AIRFLOW_UID`

3. **Construir las imÃ¡genes de Docker**
   ```bash
   docker-compose build
   ```

4. **Iniciar todos los servicios**
   ```bash
   docker-compose up -d
   ```

5. **Verificar que los contenedores estÃ©n ejecutÃ¡ndose**
   ```bash
   docker-compose ps
   ```

6. **Acceder a la interfaz de Airflow**
   - URL: http://localhost:8080
   - Usuario: `airflow`
   - ContraseÃ±a: `airflow`

7. **Acceder a la Sales API**
   - URL: http://localhost:8081
   - Endpoint para generar ventas: `POST http://localhost:8081/api/sales`

### Comandos Ãštiles

```bash
# Ver logs de todos los servicios
docker-compose logs -f

# Ver logs de un servicio especÃ­fico
docker-compose logs -f airflow-scheduler

# Detener todos los servicios
docker-compose down

# Detener y eliminar volÃºmenes (reinicio completo)
docker-compose down -v

# Reiniciar un servicio especÃ­fico
docker-compose restart airflow-webserver
```

---

## ğŸ“¸ Evidencia del Proyecto

### 1. Sales API - EnvÃ­o de Ã“rdenes

Captura de Swagger UI (`/docs`) o Postman mostrando una respuesta exitosa (200 OK) al enviar una orden de venta.

![API - Respuesta exitosa](./screenshots/API.jpg)

---

### 2. Interfaz de Airflow - Vista del DAG

Captura de la vista Grid/Graph del DAG `sales_pipeline_gcp` con todas las tareas en verde (Success).

![Airflow - DAG en Success](./screenshots/Airflow_2.jpg)

---

### 3. Logs de Procesamiento en Airflow

Captura de los logs de la tarea `process_and_upload_gcs` mostrando que procesÃ³ mensajes correctamente.

![Airflow - Logs de procesamiento](./screenshots/Airflow_1.jpg)

---

### 4. Google Cloud Platform - Pub/Sub

Captura de la consola de Pub/Sub mostrando el Topic `sales-topic` y la Subscription `sales-sub` creados.

![GCP - Pub/Sub Console](./screenshots/GCP_1.jpg)

---

### 5. Google Cloud Platform - BigQuery

Captura de BigQuery mostrando el esquema de la tabla `sales_summary` creada y una consulta (SELECT *) con los datos cargados.

![GCP - BigQuery Schema y Datos](./screenshots/GCP_2.jpg)

---

## ğŸ› ï¸ TecnologÃ­as Utilizadas

- **Apache Airflow 2.6.2**: OrquestaciÃ³n de workflows
- **Docker & Docker Compose**: ContainerizaciÃ³n y orquestaciÃ³n de servicios
- **Google Cloud Pub/Sub**: Sistema de mensajerÃ­a
- **Google Cloud Storage**: Almacenamiento de datos
- **Google BigQuery**: Data Warehouse
- **PostgreSQL**: Base de datos para metadatos de Airflow
- **Redis**: Backend de Celery para ejecuciÃ³n distribuida
- **Flask**: Framework para la Sales API

---

## ğŸ“ Estructura del Proyecto

```
lab_final/
â”œâ”€â”€ .env                        # Variables de entorno
â”œâ”€â”€ docker-compose.yaml         # ConfiguraciÃ³n de servicios
â”œâ”€â”€ Dockerfile                  # Imagen personalizada de Airflow
â”œâ”€â”€ google_credentials.json     # Credenciales de GCP
â”œâ”€â”€ app/                        # Sales API
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ dags/                       # DAGs de Airflow
â”‚   â””â”€â”€ gcp_pipeline_dag.py     # Pipeline principal
â”œâ”€â”€ logs/                       # Logs de Airflow
â”œâ”€â”€ plugins/                    # Plugins personalizados
â””â”€â”€ config/                     # Configuraciones adicionales
```

---

## ğŸ“ Notas Adicionales

- El pipeline se ejecuta automÃ¡ticamente cada hora (`@hourly`)
- Se pueden generar ventas manualmente usando la Sales API
- Los datos se procesan en formato NDJSON para compatibilidad con BigQuery
- Las tablas en BigQuery se crean automÃ¡ticamente si no existen
- La tabla `sales_summary` se recrea completamente en cada ejecuciÃ³n (CREATE OR REPLACE)

---

## ğŸ¯ Objetivos de Aprendizaje Logrados

âœ… ImplementaciÃ³n de un pipeline de datos end-to-end  
âœ… OrquestaciÃ³n con Apache Airflow en Docker  
âœ… IntegraciÃ³n con servicios de Google Cloud Platform  
âœ… Procesamiento de mensajes en streaming (Pub/Sub)  
âœ… Almacenamiento y transformaciÃ³n en BigQuery  
âœ… AutomatizaciÃ³n de workflows de datos  

---

**Proyecto desarrollado para el MÃ³dulo 6 - IngenierÃ­a de Datos Avanzada con Python**